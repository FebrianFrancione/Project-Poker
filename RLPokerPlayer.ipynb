{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pypokerengine.players import BasePokerPlayer\n",
    "from pypokerengine.utils.card_utils import gen_cards, estimate_hole_card_win_rate\n",
    "import math\n",
    "import random\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import namedtuple\n",
    "from itertools import count\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as T\n",
    "\n",
    "# Here we build the Neural Network to use for our Policy Network and Target Network\n",
    "# 2 Fully Connected Hidden Layers\n",
    "# 1 Output Layer\n",
    "# We input the game state\n",
    "# Output Layer has 4 possible outcomes [CALL, FOLD, RAISE_MIN, RAISE_MAX]\n",
    "\n",
    "class DQN(nn.Module):\n",
    "    # need img height and width\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(in_features=1, out_features=24)\n",
    "        self.fc2 = nn.Linear(in_features=24, out_features=32)\n",
    "        self.out = nn.Linear(in_features=32, out_features=4)\n",
    "\n",
    "    # Forward pass to network, esentially passing tensor t through the network, standard implementation\n",
    "    def forward(self, t):\n",
    "        t = t.flatten(start_dim=1)\n",
    "        t = F.relu(self.fc1(t))\n",
    "        t = F.relu(self.fc2(t))\n",
    "        t = self.out(t)\n",
    "        return t\n",
    "\n",
    "\n",
    "# Experience Class, here we define the Experience Object to be used for our Replay Memory\n",
    "Experience = namedtuple('Experience', ('state', 'action', 'next_state', 'reward'))\n",
    "\n",
    "\n",
    "# The Class we use to store Experience Memory\n",
    "class ReplayMemory:\n",
    "    def __init__(self, capacity):\n",
    "        self.capacity = capacity\n",
    "        self.memory = []\n",
    "        self.push_count = 0\n",
    "\n",
    "    # add experience but check the memory isnt at capacity, else put the new memories in front\n",
    "    def push(self, experience):\n",
    "        if len(self.memory) < self.capacity:\n",
    "            self.memory.append(experience)\n",
    "        else:\n",
    "            self.memory[self.push_count % self.capacity] = experience\n",
    "        self.push_count += 1\n",
    "\n",
    "    # what we use to train DQN, returns random sample of experiences\n",
    "    def sample(self, batch_size):\n",
    "        return random.sample(self.memory, batch_size)\n",
    "\n",
    "    # returns boolean to tell if we can sample from memory\n",
    "    def can_provide_sample(self, batch_size):\n",
    "        return len(self.memory) >= batch_size\n",
    "\n",
    "\n",
    "# exploration = agent randomly exploring\n",
    "# exploitation = agent exploits what it's learned to take best action\n",
    "# epsilon greedy strategy to balance between both\n",
    "# exploration becomes less probable the more the agent explores and learns about it's environment\n",
    "class EpsilonGreedyStrategy:\n",
    "    def __init__(self, start, end, decay):\n",
    "        self.start = start\n",
    "        self.end = end\n",
    "        self.decay = decay\n",
    "\n",
    "    # The formula for Epsilon Greedy but in code\n",
    "    def get_exploration_rate(self, current_step):\n",
    "        return self.end + (self.start - self.end) * \\\n",
    "               math.exp(-1. * current_step * self.decay)\n",
    "\n",
    "    # strategy and number of available actions, implement this in RlPlayer\n",
    "\n",
    "\n",
    "class Agent:\n",
    "    def __init__(self, strategy, num_actions, device):\n",
    "        self.current_step = 0\n",
    "        self.strategy = strategy\n",
    "        self.num_actions = num_actions\n",
    "        self.device = device\n",
    "\n",
    "    # policy_net = deep Q network\n",
    "    # rate is what we use from epsilon greedy\n",
    "    def select_action(self, state, policy_net):\n",
    "        rate = self.strategy.get_exploration_rate(self.current_step)\n",
    "        self.current_step += 1\n",
    "\n",
    "        # pick action based on rate\n",
    "        if rate > random.random():\n",
    "            action = random.randrange(self.num_actions)\n",
    "            return torch.tensor([action]).to(self.device)  # explore\n",
    "        else:\n",
    "            with torch.no_grad():\n",
    "                return policy_net(state).argmax(dim=1).to(self.device)  # exploit\n",
    "\n",
    "def extract_tensors(experiences):\n",
    "    # Convert batch of Experiences to Experience of batches\n",
    "    batch = Experience(*zip(*experiences))\n",
    "\n",
    "    t1 = torch.cat(batch.state)\n",
    "    t2 = torch.cat(batch.action)\n",
    "    t3 = torch.cat(batch.reward)\n",
    "    t4 = torch.cat(batch.next_state)\n",
    "\n",
    "    return (t1,t2,t3,t4)\n",
    "\n",
    "class QValues:\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    @staticmethod\n",
    "    def get_current(policy_net, states, actions):\n",
    "        return policy_net(states).gather(dim=1, index=actions.unsqueeze(-1))\n",
    "\n",
    "    @staticmethod\n",
    "    def get_next(target_net, next_states):\n",
    "        final_state_locations = next_states.flatten(start_dim=1) \\\n",
    "            .max(dim=1)[0].eq(0).type(torch.bool)\n",
    "        non_final_state_locations = (final_state_locations == False)\n",
    "        non_final_states = next_states[non_final_state_locations]\n",
    "        batch_size = next_states.shape[0]\n",
    "        values = torch.zeros(batch_size).to(QValues.device)\n",
    "        values[non_final_state_locations] = target_net(non_final_states).max(dim=1)[0].detach()\n",
    "        return values\n",
    "\n",
    "\n",
    "# strategy and number of available actions\n",
    "class Agent:\n",
    "    def __init__(self, strategy, num_actions, device):\n",
    "        self.current_step = 0\n",
    "        self.strategy = strategy\n",
    "        self.num_actions = num_actions\n",
    "        self.device = device\n",
    "\n",
    "    # policy_net = deep Q network\n",
    "    # rate is what we use from epsilon greedy\n",
    "    def select_action(self, win_rate, policy_net):\n",
    "        rate = self.strategy.get_exploration_rate(self.current_step)\n",
    "        self.current_step += 1\n",
    "\n",
    "        # pick action based on rate\n",
    "        if rate > random.random():\n",
    "            action = random.randrange(self.num_actions)\n",
    "            return torch.tensor([action]).to(self.device)  # explore\n",
    "        else:\n",
    "            with torch.no_grad():\n",
    "                return policy_net(win_rate).argmax(dim=1).to(self.device)  # exploit\n",
    "\n",
    "class RLPokerAgent(BasePokerPlayer):\n",
    "    # hyperparameters and other setup for RL Player\n",
    "    def __init__(self):\n",
    "        self.batch_size = 256\n",
    "        self.gamma = 0.999\n",
    "        self.eps_start = 1\n",
    "        self.eps_end = 0.01\n",
    "        self.eps_decay = 0.001\n",
    "        self.target_update = 10\n",
    "        self.memory_size = 100000\n",
    "        self.lr = 0.001\n",
    "        self.episode_reward_amount = []\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.strategy = EpsilonGreedyStrategy(self.eps_start, self.eps_end, self.eps_decay)\n",
    "        self.memory = ReplayMemory(self.memory_size)\n",
    "        self.policy_net = DQN().to(self.device)\n",
    "        self.target_net = DQN().to(self.device)\n",
    "        self.target_net.load_state_dict(self.policy_net.state_dict())\n",
    "\n",
    "\n",
    "        # this network is not in training mode\n",
    "        self.target_net.eval()\n",
    "\n",
    "        # pass policy network\n",
    "        self.optimizer = optim.Adam(params=self.policy_net.parameters(), lr=self.lr)\n",
    "\n",
    "        #storing the variable we need for the Q table\n",
    "        self.state = 0\n",
    "        self.next_state = 0\n",
    "        self.last_action = []\n",
    "        self.reward = 0\n",
    "\n",
    "    def declare_action(self, valid_actions, hole_card, round_state):\n",
    "        community_card = round_state['community_card']\n",
    "        win_rate = estimate_hole_card_win_rate(\n",
    "            nb_simulation=1000,\n",
    "            nb_player=2,\n",
    "            hole_card=gen_cards(hole_card),\n",
    "            community_card=gen_cards(community_card)\n",
    "        )\n",
    "\n",
    "        agent = Agent(self.strategy, 4, self.device)\n",
    "        action = agent.select_action(win_rate, self.policy_net)\n",
    "\n",
    "        self.last_action = action\n",
    "        self.state = self.next_state\n",
    "        self.next_state = win_rate\n",
    "\n",
    "        if action == 0:\n",
    "            action_to_execute = valid_actions[0]\n",
    "            print(\"************RL Player: Fold************\")\n",
    "            return action_to_execute['action'], action_to_execute['amount']\n",
    "        elif action == 1:\n",
    "            action_to_execute = valid_actions[1]\n",
    "            print(\"************ RL Player: Call ************\")\n",
    "            return action_to_execute['action'], action_to_execute['amount']\n",
    "        elif action == 2:\n",
    "            action_to_execute = valid_actions[2]\n",
    "            print(\"************ RL Player: Raise Min ************\")\n",
    "            print(action_to_execute['amount']['min'])\n",
    "            print(action_to_execute['amount']['max'])\n",
    "            return action_to_execute['action'], action_to_execute['amount']['min']\n",
    "        elif action == 3:\n",
    "            action_to_execute = valid_actions[2]\n",
    "            print(\"************ RL Player: Raise Max ************\")\n",
    "            return action_to_execute['action'], action_to_execute['amount']['max']\n",
    "\n",
    "\n",
    "    def receive_game_start_message(self, game_info):\n",
    "        pass\n",
    "\n",
    "    def receive_round_start_message(self, round_count, hole_card, seats):\n",
    "        pass\n",
    "\n",
    "    def receive_street_start_message(self, street, round_state):\n",
    "        pass\n",
    "\n",
    "    def receive_game_update_message(self, action, round_state):\n",
    "        pass\n",
    "\n",
    "    def get_stack(self, round_state):\n",
    "        uuid = self.uuid\n",
    "        seats = round_state['seats']\n",
    "        stack_amount = 0\n",
    "        for player in seats:\n",
    "            if player['uuid'] == uuid:\n",
    "                stack_amount = player['stack']\n",
    "                break\n",
    "        return stack_amount\n",
    "\n",
    "    # This is where the round ends so we update our Experience and Update QValues here\n",
    "    def receive_round_result_message(self, winners, hand_info, round_state):\n",
    "\n",
    "        reward = self.get_stack(round_state)\n",
    "        print(reward)\n",
    "        action = self.last_action\n",
    "        # Reward = Stack size\n",
    "        state = self.state\n",
    "        next_state = self.next_state\n",
    "        self.memory.push(Experience(state, action, next_state, reward))\n",
    "        state = next_state\n",
    "\n",
    "        if self.memory.can_provide_sample(self.batch_size):\n",
    "            experiences = self.memory.sample(self.batch_size)\n",
    "            states, actions, rewards, next_states = extract_tensors(experiences)\n",
    "\n",
    "            current_q_values = QValues.get_current(self.policy_net, states, actions)\n",
    "            next_q_values = QValues.get_next(self.target_net, next_states)\n",
    "            target_q_values = (next_q_values * self.gamma) + rewards\n",
    "\n",
    "            loss = F.mse_loss(current_q_values, target_q_values.unsqueeze(1))\n",
    "            self.optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            self.optimizer.step()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
